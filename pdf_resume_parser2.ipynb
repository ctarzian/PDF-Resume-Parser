{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Resume Parser: Turning Unstructured Data into a Dataset\n",
    "### Chris Tarzian\n",
    "##### 6/23/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The code below was built to turn unstructured data into a structured dataset. It works to parse through a candidate's resume and returns their Full Name, Cell Phone, Email, Location, College, Education Level, Company, Job Title and Skillset. <br><br>I utilized various NLP techniques throughout including n-grams, punctuation removal, tokenization, text-cleaning, and the removing of stop-words. I also implemented web scraping using Selenium and Beautiful Soup to get access to valuable information such as Company names and Colleges, so as to build an archive to help identify information in the resumes. <br><br>I was inspired to do this project as my work as a recruiter requires me to sift through resumes all the time and identify high-priority candidates. This was my first foray into turning unstructured data into a dataset and I welcome any feedback or insight to improve the code. Cheers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os import walk\n",
    "\n",
    "import io\n",
    "import re\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "\n",
    "import spacy\n",
    "import locationtagger\n",
    " \n",
    "# essential entity models downloads\n",
    "# nltk.downloader.download('maxent_ne_chunker')\n",
    "# nltk.downloader.download('words')\n",
    "# nltk.downloader.download('treebank')\n",
    "# nltk.downloader.download('maxent_treebank_pos_tagger')\n",
    "# nltk.downloader.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from pdfminer.high_level import extract_text\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code below to locate absolute path of a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chris Tarzian\\Documents\\Post-Fordham\\Visual Studio Code\\PDF Resume Parser\\ChrisTarzianResume2022.pdf\n"
     ]
    }
   ],
   "source": [
    "simple_path = 'ChrisTarzianResume2022.pdf'\n",
    "abs_path = os.path.abspath(simple_path)\n",
    "print(abs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Chris Tarzian\\\\Documents\\\\Post-Fordham\\\\Visual Studio Code\\\\PDF Resume Parser\\\\Resumes2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##r is ver important here\n",
    "## setting the working directory\n",
    "os.chdir(r\"c:\\Users\\Chris Tarzian\\Documents\\Post-Fordham\\Visual Studio Code\\PDF Resume Parser\\Resumes2\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:Green '> Importing PDF Files/Resumes </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aram_Keshgegian_Resume.pdf',\n",
       " 'ArbyTorossianResume.pdf',\n",
       " 'ChrisTarzianResume2022.pdf',\n",
       " 'DeanDerSimonianResume.pdf',\n",
       " 'MT Resume-1.pdf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting resume files as all the pdf's inside the directory\n",
    "resume_files = [file for file in os.listdir('.') if os.path.isfile(file) and file.endswith('.pdf')]\n",
    "resume_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = extract_text('ChrisTarzianResume2022.pdf')\n",
    "text2 = extract_text('DeanDersimonianResume.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:Green '> Get Full Name Function </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_name(name):   \n",
    "    #text = name.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r\"[,.;/@#?!&$]+\\ *\", \" \", name)\n",
    "        ## lower extracted text\n",
    "    text = text.lower()\n",
    "\n",
    "    ngram_list = [2,3]\n",
    "    name_grams= []\n",
    "\n",
    "    for i in ngram_list:\n",
    "            ## creating ngrams\n",
    "            grams = list(ngrams([token for token in text.split(\" \") if len(token) > 1],i))\n",
    "            ## looping through to match bigrams\n",
    "            for g in grams:\n",
    "                    grams = list(map(' '.join, nltk.everygrams(g, i)))\n",
    "                    grams = [each_string.lower() for each_string in grams]\n",
    "                    name_grams.append(grams[0].title())\n",
    "                    break\n",
    "    \n",
    "    return name_grams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chris Tarzian'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_full_name(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:Green '> Get Phone Number Function </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phone_number(text):\n",
    "    ### define phone number input\n",
    "    phone_number = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
    "    \n",
    "    if phone_number:\n",
    "        number = ''.join(phone_number[0])\n",
    "        if len(number) > 10:\n",
    "            return '+' + number\n",
    "        else:\n",
    "            return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019567519'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_phone_number(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:Green '> Get Email Function </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email(email):\n",
    "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\n",
    "    if email:\n",
    "        try:\n",
    "            return email[0].split()[0].strip(';')\n",
    "        except IndexError:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'christarzian@gmail.com'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_email(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:Green '> Get Location Function </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(location):   \n",
    "    # extracting entities.\n",
    "    place_entity = locationtagger.find_locations(text = location)\n",
    "\n",
    "    # initializing a list\n",
    "    location_list = []\n",
    "\n",
    "    ## appending both types of cities\n",
    "    location_list.append(place_entity.regions)\n",
    "    location_list.append(place_entity.region_cities)\n",
    "\n",
    "    ## creating a loop to identify if its a dictionary or a list and taking the first instance\n",
    "    if len(location_list[0]) == 0:\n",
    "        location_list = location_list[1]\n",
    "        location_list = list(location_list.keys())[0]\n",
    "    else:\n",
    "        location_list = location_list[0]\n",
    "        location_list = ' '.join(location_list)\n",
    "\n",
    "    return location_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New York'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_location(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:Green '> Get Education Level Function </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edu_level(edu_level):\n",
    "        ## list of types of degree options\n",
    "        edu_level_list = ['BA','BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "                    'MS', 'M.S.', 'MBA', 'M.B.A.', 'M.B.A'\n",
    "                    'phd', 'PhD', 'Ph.D.', 'Ph.D','MSEI',\n",
    "                    'BACHELOR OF SCIENCES', 'BACHELOR OF SCIENCE', 'BACHELOR OF ARTS', 'BACHELOR OF ENGINEERING', \n",
    "                    'BACHELOR OF MUSIC', 'BACHELOR OF EDUCATION', 'BACHELORS OF BUSINESS ADMINISTRATION',\n",
    "                    'BACHELOR OF BUSINESS ADMINISTRATION', 'BACHELOR IN BUSINESS ADMINISTRATION', 'BACHELORS IN BUSINESS ADMINISTRATION',\n",
    "                    'MASTER OF MUSIC', 'MASTER OF EDUCATION', 'MASTER OF SCIENCE', \n",
    "                    'MASTERS', 'MASTER OF ACCOUNTING','MASTER OF BUSINESS ADMINISTRATION']\n",
    "                    \n",
    "        ## cleaning text of punctuation            \n",
    "        text = re.sub(r\"[,.;/@#?!&$]+\\ *\", \" \", edu_level)\n",
    "        ## creating list of ngram numbers to create\n",
    "        ngram_list = [1,2,3,4]\n",
    "        ## creating a set to store all final ngram values\n",
    "        edu_level_grams = set()\n",
    "        for i in ngram_list:\n",
    "                ## creating ngrams, iterating through ngram list\n",
    "                edu_grams = ngrams(text.split(), i)\n",
    "                ## looping through to match grams\n",
    "                for grams in edu_grams:\n",
    "                        ## joining each ngram, iterating through ngram list\n",
    "                        edu_grams = list(map(' '.join, nltk.everygrams(grams, i)))\n",
    "                        edu_grams = [each_string.upper() for each_string in edu_grams]\n",
    "                        for grams in edu_grams:\n",
    "                                ## if grams in both lists appending to set\n",
    "                                if grams in edu_level_list:\n",
    "                                        edu_level_grams.add(grams)\n",
    "\n",
    "        # convert Set to String\n",
    "        edu_level_grams = ', '.join(edu_level_grams)\n",
    "\n",
    "        return edu_level_grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MASTER OF SCIENCE, BACHELOR OF BUSINESS ADMINISTRATION'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_edu_level(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:Green '> Get Skills Function </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:Green '> Web Scraping for Skills </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: web scrape code below is current but companies can change their HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# providing url\n",
    "url = 'https://www.jobscan.co/blog/top-resume-keywords-boost-resume/'\n",
    "  \n",
    "# creating request object\n",
    "req = requests.get(url)\n",
    "  \n",
    "# creating soup object\n",
    "data = BeautifulSoup(req.text, 'html')\n",
    "\n",
    "##initializing list to store skills\n",
    "skills_list = []\n",
    "\n",
    "# finding all li tags in ol\n",
    "data1 = data.find('ol')\n",
    "for li in data1.find_all(\"li\"):\n",
    "    skills_list.append(li.text)\n",
    "\n",
    "data2 = data.find('ol', start='26')\n",
    "for li in data2.find_all(\"li\"):\n",
    "    skills_list.append(li.text)\n",
    "\n",
    "data3 = data.find('ol', start='51')\n",
    "for li in data3.find_all(\"li\"):\n",
    "    skills_list.append(li.text)\n",
    "\n",
    "data4 = data.find('ol', start='76')\n",
    "for li in data4.find_all(\"li\"):\n",
    "    skills_list.append(li.text)\n",
    "\n",
    "data5 = data.find('ol', start='101')\n",
    "for li in data5.find_all(\"li\"):\n",
    "    skills_list.append(li.text)\n",
    "\n",
    "data6 = data.find('ol', start='126')\n",
    "for li in data6.find_all(\"li\"):\n",
    "    skills_list.append(li.text)\n",
    "\n",
    "data7 = data.find('ol', start='151')\n",
    "for li in data7.find_all(\"li\"):\n",
    "    skills_list.append(li.text)\n",
    "\n",
    "data8 = data.find('ol', start='326')\n",
    "for li in data8.find_all(\"li\"):\n",
    "    skills_list.append(li.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking to see length is correct, scraped 500 skills\n",
    "print(len(skills_list))\n",
    "del skills_list[0]\n",
    "skills_df = pd.DataFrame(skills_list, columns=['Skills'])\n",
    "skills_df.to_csv('skills_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_df = pd.read_csv('skills_list.csv')\n",
    "skills_list = skills_df['Skills'].tolist()\n",
    "skills_list = [each_string.lower() for each_string in skills_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:Green '> Skills Function </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skills(skill):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(skill)\n",
    " \n",
    "    # remove the stop words\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    " \n",
    "    # remove the punctuation\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    " \n",
    "    # generate bigrams and trigrams (such as artificial intelligence)\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    " \n",
    "    # we create a set to keep the results in.\n",
    "    found_skills = set()\n",
    "  \n",
    "    # we search for each token in our skills database\n",
    "    for token in filtered_tokens:\n",
    "        if token.lower() in skills_list:\n",
    "            found_skills.add(token.title())\n",
    " \n",
    "    # we search for each bigram and trigram in our skills database\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram.lower() in skills_list:\n",
    "            found_skills.add(ngram.title())\n",
    "    \n",
    "    # convert Set to String\n",
    "    found_skills = ', '.join(found_skills)\n",
    "\n",
    "    return found_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine Learning, Mining, Mis, Python, Sports, Sourcing, Analytics, Technical Skills, Modeling, Linux, Recruiting, Technical, Sql, Data Analysis, Social Media, Tableau, Researching, Content, Analysis, Programming, Marketing, Finance'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_skills(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:Green '> Get College Function </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:Green '> Web Scraping College/Universities </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: web scrape code below is current but companies can change their HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# providing url\n",
    "url2 = 'https://www.4icu.org/us/a-z/'\n",
    "  \n",
    "# creating request object\n",
    "req2 = requests.get(url2)\n",
    "  \n",
    "# creating soup object\n",
    "soup = BeautifulSoup(req2.text, 'html')\n",
    "\n",
    "col_data = soup.find_all(\"a\")\n",
    "college_list = []\n",
    "\n",
    "for col in col_data:\n",
    "    college_list.append(col.text)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(college_list.index('A.T. Still University'))\n",
    "# print(college_list.index('Youngstown State University'))\n",
    "college_list = college_list[52:1817]\n",
    "college_df = pd.DataFrame(college_list, columns=['Universities'])\n",
    "college_df.to_csv('college_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_df = pd.read_csv('college_list.csv')\n",
    "college_list = college_df['Universities'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_list = [each_string.lower() for each_string in college_list]\n",
    "college_list[329] = 'columbia university'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:Green '> College Function </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_college(college):\n",
    "        #text = college.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = re.sub(r\"[,.;/@#?!&$]+\\ *\", \" \", college)\n",
    "        ## lower extracted text\n",
    "        text = text.lower()\n",
    "\n",
    "        ngram_list = [2,3,4,5,6,7]\n",
    "        college_grams= []\n",
    "\n",
    "        for i in ngram_list:\n",
    "                ## creating ngrams\n",
    "                grams = ngrams(text.split(), i)\n",
    "                ## looping through to match bigrams\n",
    "                for g in grams:\n",
    "                        grams = list(map(' '.join, nltk.everygrams(g, i)))\n",
    "                        grams = [each_string.lower() for each_string in grams]\n",
    "                        for g in grams:\n",
    "                                if g in college_list:\n",
    "                                        college_grams.append(g.title())\n",
    "\n",
    "        # convert Set to String\n",
    "        college_grams = ', '.join(college_grams)\n",
    "\n",
    "        return college_grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fordham University, Villanova University'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_college(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:Green '> Get Job Title Function </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:Green '> Web Scraping CareerBuilder for Job Titles </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: web scrape code below is current but companies can change their HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_path = r'c:/Users/Chris Tarzian/Desktop/chromedriver.exe'\n",
    "driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "# providing url\n",
    "url3 = 'https://www.careerbuilder.com/browse'\n",
    "driver.get(url3)\n",
    "\n",
    "alphabet = list(string.ascii_lowercase)\n",
    "job_title_list = []\n",
    "\n",
    "for i in alphabet:   \n",
    "    \n",
    "    driver.find_element_by_xpath(f\"//a[@href='/browse/titles/{i}']\").click()\n",
    "   \n",
    "    page_source = driver.page_source\n",
    "    data3 = BeautifulSoup(page_source, 'html')\n",
    "\n",
    "    jobtitle_content = data3.find_all('a', class_='col-mobile-full')\n",
    "\n",
    "    for k in jobtitle_content:\n",
    "        job_title_list.append(k.text)\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title_df = pd.DataFrame(job_title_list, columns=['Job Title'])\n",
    "job_title_df.to_csv('job_title_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title_df = pd.read_csv('job_title_list.csv')\n",
    "job_title_list = job_title_df['Job Title'].tolist()\n",
    "job_title_list = [each_string.lower() for each_string in job_title_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:Green '> Job Title Function </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_title(title):\n",
    "        text = re.sub(r\"[,.;/@#?!&$]+\\ *\", \" \", title)\n",
    "        ## lower extracted text\n",
    "        text = text.lower()\n",
    "\n",
    "        ngram_list = [1,2,3]\n",
    "        job_title_grams= set()\n",
    "\n",
    "        for i in ngram_list:\n",
    "                ## creating ngrams\n",
    "                grams = ngrams(text.split(), i)\n",
    "                ## looping through to match bigrams\n",
    "                for g in grams:\n",
    "                        grams = list(map(' '.join, nltk.everygrams(g, i)))\n",
    "                        grams = [each_string.lower() for each_string in grams]\n",
    "                        for g in grams:\n",
    "                                if g in job_title_list:\n",
    "                                        job_title_grams.add(g.title())\n",
    "        \n",
    "        # convert Set to String\n",
    "        job_title_grams = ', '.join(job_title_grams)\n",
    "                                      \n",
    "        return job_title_grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Administration, Data Analyst, Financial Analyst, Recruiter, Finance, Technical, Business, Education, Financial, School, Senior Recruiter, Director, Ceo, Marketing, Analyst, Analytics, Science'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_job_title(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:Green '> Get Company Function </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:Green '> Web Scraping LinkedIn for Public Companies </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: web scrape code below is current but companies can change their HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_path = r'c:/Users/Chris Tarzian/Desktop/chromedriver.exe'\n",
    "driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "# providing url\n",
    "url4 = 'https://www.linkedin.com/directory/companies'\n",
    "driver.get(url4)\n",
    "\n",
    "driver.get('https://www.linkedin.com/login')\n",
    "# my secret credentials:\n",
    "email = \"######@gmail.com\"\n",
    "password = \"########\"\n",
    "# Go to linkedin and login\n",
    "\n",
    "time.sleep(3)\n",
    "driver.find_element_by_id('username').send_keys(email)\n",
    "driver.find_element_by_id('password').send_keys(password)\n",
    "driver.find_element_by_id('password').send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "url4 = 'https://www.linkedin.com/directory/companies'\n",
    "driver.get(url4)\n",
    "\n",
    "company_list = []\n",
    "alphabet = list(string.ascii_lowercase)\n",
    "\n",
    "for i in alphabet:\n",
    "    driver.find_element_by_xpath(f\"//a[@href='https://www.linkedin.com/directory/companies/{i}?trk=companies_directory_letter_nav']\").click()\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    data4 = BeautifulSoup(page_source, 'html')\n",
    "\n",
    "    company_content = data4.find_all('a', class_='listings__entry-link')\n",
    "\n",
    "    for k in company_content:\n",
    "        company_list.append(k.text)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df = pd.DataFrame(company_list, columns=['Company'])\n",
    "company_df.to_csv('company_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df = pd.read_csv('company_list.csv')\n",
    "company_list = company_df['Company'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51993"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(company_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_list = [each_string.title() for each_string in skills_list]\n",
    "company_list = [w for w in company_list if w not in skills_list]\n",
    "skills_list = [each_string.lower() for each_string in skills_list]\n",
    "company_list.remove('Y')\n",
    "company_list.remove('X')\n",
    "company_list.remove('University')\n",
    "company_list.remove('CEO')\n",
    "company_list.remove('G2')\n",
    "company_list.remove('Hiring')\n",
    "company_list.remove('Foundation')\n",
    "company_list.remove('Digital Business')\n",
    "company_list.remove('One')\n",
    "company_list.remove('Line')\n",
    "company_list.remove('Marketing Digital')\n",
    "company_list.remove('Analysts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:Green '> Company Function </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company(company):\n",
    "        #text = .translate(str.maketrans('', '', string.punctuation))\n",
    "        text = re.sub(r\"[,.;/@#?!&$]+\\ *\", \" \", company)\n",
    "        ## lower extracted text\n",
    "        #text = company\n",
    "\n",
    "        ngram_list = [1,2,3,4,5]\n",
    "        company_grams = set()\n",
    "\n",
    "        for i in ngram_list:\n",
    "                ## creating ngrams\n",
    "                grams = ngrams(text.split(), i)\n",
    "                ## looping through to match bigrams\n",
    "                for g in grams:\n",
    "                        grams = list(map(' '.join, nltk.everygrams(g, i)))\n",
    "                        #grams = [each_string.title() for each_string in grams]\n",
    "                        for g in grams:\n",
    "                                if g in company_list:\n",
    "                                        company_grams.add(g) \n",
    "        # convert Set to String\n",
    "        company_grams = ', '.join(company_grams)\n",
    "        \n",
    "        return company_grams                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deloitte, Recruiter, Sapient, Chase, EY, YOH, Figma, HBO, Razorfish, Spotify, Verizon, LinkedIn'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_company(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:Green '> Combining All Functions </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class all_funcs():\n",
    "    \n",
    "    ## get full name function         \n",
    "    def get_full_name(name):   \n",
    "        text = re.sub(r\"[,.;/@#?!&$]+\\ *\", \" \", name)\n",
    "        ## lower extracted text\n",
    "        text = text.lower()\n",
    "\n",
    "        ngram_list = [2,3]\n",
    "        name_grams= []\n",
    "\n",
    "        for i in ngram_list:\n",
    "                ## to remove middle initial of name, keeping tokens longer than 1 initial and creating ngrams\n",
    "                grams = list(ngrams([token for token in text.split(\" \") if len(token) > 1],i))\n",
    "                ## looping through to match bigrams\n",
    "                for g in grams:\n",
    "                        ## joining ngrams, and iterating through all option lengths in ngram_list\n",
    "                        grams = list(map(' '.join, nltk.everygrams(g, i)))\n",
    "                        grams = [each_string.lower() for each_string in grams]\n",
    "                        name_grams.append(grams[0].title())\n",
    "                        break\n",
    "\n",
    "        return name_grams[0]\n",
    "            \n",
    "    ## get phone number function\n",
    "    def get_phone_number(phone):\n",
    "        ### define phone number input\n",
    "        phone_number = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), phone)\n",
    "        \n",
    "        if phone_number:\n",
    "            number = ''.join(phone_number[0])\n",
    "            if len(number) > 10:\n",
    "                return '+' + number\n",
    "            else:\n",
    "                return number\n",
    "\n",
    "    ## Get email function\n",
    "    def get_email(email):\n",
    "            email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\n",
    "            if email:\n",
    "                try:\n",
    "                    return email[0].split()[0].strip(';')\n",
    "                except IndexError:\n",
    "                    return None\n",
    "\n",
    "    ## Get Location Function                \n",
    "    def get_location(location):   \n",
    "        # extracting entities.\n",
    "        place_entity = locationtagger.find_locations(text = location)\n",
    "\n",
    "        # initializing a list\n",
    "        location_list = []\n",
    "\n",
    "        ## appending both types of cities\n",
    "        location_list.append(place_entity.regions)\n",
    "        location_list.append(place_entity.region_cities)\n",
    "\n",
    "        ## creating a loop to identify if its a dictionary or a list and taking the first instance\n",
    "        if len(location_list[0]) == 0:\n",
    "            location_list = location_list[1]\n",
    "            location_list = list(location_list.keys())[0]\n",
    "        else:\n",
    "            location_list = location_list[0]\n",
    "            location_list = ' '.join(location_list)\n",
    "\n",
    "        return location_list\n",
    "        \n",
    "    ## Get College function\n",
    "    def get_college(college):\n",
    "        text = re.sub(r\"[,.;/@#?!&$]+\\ *\", \" \", college)\n",
    "        ## lower extracted text\n",
    "        text = text.lower()\n",
    "\n",
    "        ngram_list = [2,3,4,5,6,7]\n",
    "        college_grams = set()\n",
    "\n",
    "        for i in ngram_list:\n",
    "                ## creating ngrams\n",
    "                grams = list(ngrams(text.split(), i))\n",
    "                ## looping through to match grams\n",
    "                for g in grams:\n",
    "                         ## joining ngrams, and iterating through all option lengths in ngram_list\n",
    "                        grams = list(map(' '.join, nltk.everygrams(g, i)))\n",
    "                        grams = [each_string.lower() for each_string in grams]\n",
    "                        for g in grams:\n",
    "                                ## if ngram in college list, then adding to college grams set\n",
    "                                if g in college_list:\n",
    "                                        college_grams.add(g.title())\n",
    "\n",
    "        ## convert Set to String\n",
    "        college_grams = ', '.join(college_grams)\n",
    "\n",
    "        return college_grams \n",
    "        \n",
    "    ## Get Education Level Function\n",
    "    def get_edu_level(edu_level):\n",
    "        ## list of types of degree options\n",
    "        edu_level_list = ['BA','B.A.','B.E.', 'B.E', 'BS', 'B.S', 'B.S.',\n",
    "                    'MS', 'M.S.', 'MBA', 'M.B.A.', 'M.B.A'\n",
    "                    'phd', 'PhD', 'Ph.D.', 'Ph.D','MSEI',\n",
    "                    'BACHELOR OF SCIENCES', 'BACHELOR OF SCIENCE', 'BACHELOR OF ARTS', 'BACHELOR OF ENGINEERING', \n",
    "                    'BACHELOR OF MUSIC', 'BACHELOR OF EDUCATION', 'BACHELORS OF BUSINESS ADMINISTRATION',\n",
    "                    'BACHELOR OF BUSINESS ADMINISTRATION', 'BACHELOR IN BUSINESS ADMINISTRATION', 'BACHELORS IN BUSINESS ADMINISTRATION',\n",
    "                    'MASTER OF MUSIC', 'MASTER OF EDUCATION', 'MASTER OF SCIENCE', \n",
    "                    'MASTERS', 'MASTER OF ACCOUNTING','MASTER OF BUSINESS ADMINISTRATION']\n",
    "                    \n",
    "        ## cleaning text of punctuation            \n",
    "        text = re.sub(r\"[,;/@#?!&$]+\\ *\", \" \", edu_level)\n",
    "        ## creating list of ngram numbers to create\n",
    "        ngram_list = [1,2,3,4]\n",
    "        ## creating a set to store all final ngram values\n",
    "        edu_level_grams = set()\n",
    "        for i in ngram_list:\n",
    "                ## creating ngrams, iterating through ngram list\n",
    "                edu_grams = list(ngrams(text.split(), i))\n",
    "                ## looping through to match grams\n",
    "                for grams in edu_grams:\n",
    "                        ## joining each ngram, iterating through ngram list\n",
    "                        edu_grams = list(map(' '.join, nltk.everygrams(grams, i)))\n",
    "                        edu_grams = [each_string.upper() for each_string in edu_grams]\n",
    "                        for grams in edu_grams:\n",
    "                                ## if grams in both lists appending to set\n",
    "                                if grams in edu_level_list:\n",
    "                                        edu_level_grams.add(grams)\n",
    "        ## convert Set to String\n",
    "        edu_level_grams = ', '.join(edu_level_grams)\n",
    "\n",
    "        return edu_level_grams\n",
    "\n",
    "    ## Get Company Function\n",
    "    def get_company(company):\n",
    "        ##cleaning punctuation\n",
    "        text = re.sub(r\"[,.;/@#?!&$]+\\ *\", \" \", company)\n",
    "        \n",
    "        ngram_list = [1,2,3,4,5]\n",
    "        company_grams = set()\n",
    "\n",
    "        for i in ngram_list:\n",
    "                ## creating ngrams\n",
    "                grams = ngrams(text.split(), i)\n",
    "                ## looping through to match bigrams\n",
    "                for g in grams:\n",
    "                        grams = list(map(' '.join, nltk.everygrams(g, i)))\n",
    "                        #grams = [each_string.title() for each_string in grams]\n",
    "                        for g in grams:\n",
    "                                if g in company_list:\n",
    "                                        company_grams.add(g)\n",
    "        ## convert Set to String\n",
    "        company_grams = ', '.join(company_grams)\n",
    "        \n",
    "        return company_grams \n",
    "\n",
    "    ## Get job title function\n",
    "    def get_job_title(title):\n",
    "        text = re.sub(r\"[,.;/@#?!&$]+\\ *\", \" \", title)\n",
    "        ## lower extracted text\n",
    "        text = text.lower()\n",
    "\n",
    "        ngram_list = [2,3,4,5]\n",
    "        job_title_grams= set()\n",
    "\n",
    "        for i in ngram_list:\n",
    "                ## creating ngrams\n",
    "                grams = ngrams(text.split(), i)\n",
    "                ## looping through to match bigrams\n",
    "                for g in grams:\n",
    "                        grams = list(map(' '.join, nltk.everygrams(g, i)))\n",
    "                        grams = [each_string.lower() for each_string in grams]\n",
    "                        for g in grams:\n",
    "                                if g in job_title_list:\n",
    "                                        job_title_grams.add(g.title())\n",
    "        ## convert Set to String\n",
    "        job_title_grams = ', '.join(job_title_grams)\n",
    "                                        \n",
    "        return job_title_grams \n",
    "        \n",
    "    ## Get Skills Function\n",
    "    def get_skills(skill):\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        word_tokens = nltk.tokenize.word_tokenize(skill)\n",
    "    \n",
    "        # remove the stop words\n",
    "        filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "    \n",
    "        # remove the punctuation\n",
    "        filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "    \n",
    "        # generate bigrams and trigrams (such as artificial intelligence)\n",
    "        bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    "    \n",
    "        # we create a set to keep the results in.\n",
    "        found_skills = set()\n",
    "    \n",
    "        # we search for each token in our skills database\n",
    "        for token in filtered_tokens:\n",
    "            if token.lower() in skills_list:\n",
    "                found_skills.add(token.title())\n",
    "    \n",
    "        # we search for each bigram and trigram in our skills database\n",
    "        for ngram in bigrams_trigrams:\n",
    "            if ngram.lower() in skills_list:\n",
    "                found_skills.add(ngram.title())\n",
    "\n",
    "        ## convert Set to String\n",
    "        found_skills = ', '.join(found_skills)\n",
    "\n",
    "        return found_skills    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chris Tarzian\n",
      "2019567519\n",
      "christarzian@gmail.com\n",
      "New York\n",
      "Villanova University, Fordham University\n",
      "MASTER OF SCIENCE, BACHELOR OF BUSINESS ADMINISTRATION\n",
      "Deloitte, Recruiter, Sapient, Chase, EY, YOH, Figma, HBO, Razorfish, Spotify, Verizon, LinkedIn\n",
      "Data Analyst, Senior Recruiter, Financial Analyst\n",
      "Machine Learning, Mining, Mis, Python, Sports, Sourcing, Analytics, Technical Skills, Modeling, Linux, Recruiting, Technical, Sql, Data Analysis, Social Media, Tableau, Researching, Content, Analysis, Programming, Marketing, Finance\n"
     ]
    }
   ],
   "source": [
    "print(all_funcs.get_full_name(text1))\n",
    "print(all_funcs.get_phone_number(text1))\n",
    "print(all_funcs.get_email(text1))\n",
    "print(all_funcs.get_location(text1))\n",
    "print(all_funcs.get_college(text1))\n",
    "print(all_funcs.get_edu_level(text1))\n",
    "print(all_funcs.get_company(text1))\n",
    "print(all_funcs.get_job_title(text1))\n",
    "print(all_funcs.get_skills(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:Green '> Data In Structured Form </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_name = []\n",
    "phone_number = []\n",
    "email = []\n",
    "location = []\n",
    "education = []\n",
    "edu_level = []\n",
    "skills = []\n",
    "college = []\n",
    "job_title = []\n",
    "company = []\n",
    "\n",
    "## looping through resume directory\n",
    "for info in resume_files:\n",
    "    ## extracting text from each resume\n",
    "    text = extract_text(info)\n",
    "    ## running extracted text from all previously defined functions\n",
    "    full_name.append(all_funcs.get_full_name(text))\n",
    "    phone_number.append(all_funcs.get_phone_number(text))\n",
    "    email.append(all_funcs.get_email(text))\n",
    "    location.append(all_funcs.get_location(text))\n",
    "    edu_level.append(all_funcs.get_edu_level(text))\n",
    "    college.append(all_funcs.get_college(text))\n",
    "    company.append(all_funcs.get_company(text))\n",
    "    job_title.append(all_funcs.get_job_title(text))\n",
    "    skills.append(all_funcs.get_skills(text))\n",
    "\n",
    "#gathering data into a dictionary\n",
    "data = {\n",
    "    'Full Name':full_name,\n",
    "    'Phone_Number':phone_number,\n",
    "    'Email': email,\n",
    "    'Location': location,\n",
    "    'College':college,\n",
    "    'Education_Level' : edu_level,\n",
    "    'Company' : company,\n",
    "    'Job_Title' : job_title,\n",
    "    'Skills':skills\n",
    "}\n",
    "## converting dictionary into a df\n",
    "resume_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full Name</th>\n",
       "      <th>Phone_Number</th>\n",
       "      <th>Email</th>\n",
       "      <th>Location</th>\n",
       "      <th>College</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Company</th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aram Keshgegian</td>\n",
       "      <td>6107166226</td>\n",
       "      <td>aram.keshgegian@vertexinc.com</td>\n",
       "      <td>Yerevan</td>\n",
       "      <td>Drexel University</td>\n",
       "      <td>MASTER OF BUSINESS ADMINISTRATION, BACHELOR OF...</td>\n",
       "      <td>Mother, King, Vertex Inc, Performance Food Gro...</td>\n",
       "      <td>Inside Sales Representative, Sales Development...</td>\n",
       "      <td>Partnership, Sales, Crm, Oracle, Process Impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arby Torossian</td>\n",
       "      <td>6176788333</td>\n",
       "      <td>Arby.Torossian@gmail.com</td>\n",
       "      <td>Massachusetts New York</td>\n",
       "      <td>University Of Massachusetts Boston</td>\n",
       "      <td>B.S.</td>\n",
       "      <td>Wells Fargo, Wayfair, Canva, Toast, Looker, We...</td>\n",
       "      <td>Operations Analyst, Data Analyst, Internationa...</td>\n",
       "      <td>Operations, Sales, Transactions, Warehouse, Qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chris Tarzian</td>\n",
       "      <td>2019567519</td>\n",
       "      <td>christarzian@gmail.com</td>\n",
       "      <td>New York</td>\n",
       "      <td>Villanova University, Fordham University</td>\n",
       "      <td>MASTER OF SCIENCE, BACHELOR OF BUSINESS ADMINI...</td>\n",
       "      <td>Deloitte, Recruiter, Sapient, Chase, EY, YOH, ...</td>\n",
       "      <td>Data Analyst, Senior Recruiter, Financial Analyst</td>\n",
       "      <td>Machine Learning, Mining, Mis, Python, Sports,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dean Dersimonian</td>\n",
       "      <td>6109373511</td>\n",
       "      <td>ddersimon@gmail.com</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Ursinus College</td>\n",
       "      <td>BACHELOR OF ARTS</td>\n",
       "      <td>SEI, Outside, Vertex Inc, King</td>\n",
       "      <td>Inside Sales Representative, Data Entry, Solut...</td>\n",
       "      <td>Sales, Data Entry, Oracle, Value Proposition, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Matthew Tarzian</td>\n",
       "      <td>+0172019567518</td>\n",
       "      <td>mtarzian1@gmail.com</td>\n",
       "      <td>New York</td>\n",
       "      <td>Villanova University</td>\n",
       "      <td>B.A.</td>\n",
       "      <td>HomeAdvisor, Angi, Van Heusen, SAP, Google, LL...</td>\n",
       "      <td>Account Executive</td>\n",
       "      <td>Staffing, Sales, Crm, Oracle, Retention, Video...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Full Name    Phone_Number                          Email  \\\n",
       "0   Aram Keshgegian      6107166226  aram.keshgegian@vertexinc.com   \n",
       "1    Arby Torossian      6176788333       Arby.Torossian@gmail.com   \n",
       "2     Chris Tarzian      2019567519         christarzian@gmail.com   \n",
       "3  Dean Dersimonian      6109373511            ddersimon@gmail.com   \n",
       "4   Matthew Tarzian  +0172019567518            mtarzian1@gmail.com   \n",
       "\n",
       "                 Location                                   College  \\\n",
       "0                 Yerevan                         Drexel University   \n",
       "1  Massachusetts New York        University Of Massachusetts Boston   \n",
       "2                New York  Villanova University, Fordham University   \n",
       "3            Pennsylvania                           Ursinus College   \n",
       "4                New York                      Villanova University   \n",
       "\n",
       "                                     Education_Level  \\\n",
       "0  MASTER OF BUSINESS ADMINISTRATION, BACHELOR OF...   \n",
       "1                                               B.S.   \n",
       "2  MASTER OF SCIENCE, BACHELOR OF BUSINESS ADMINI...   \n",
       "3                                   BACHELOR OF ARTS   \n",
       "4                                               B.A.   \n",
       "\n",
       "                                             Company  \\\n",
       "0  Mother, King, Vertex Inc, Performance Food Gro...   \n",
       "1  Wells Fargo, Wayfair, Canva, Toast, Looker, We...   \n",
       "2  Deloitte, Recruiter, Sapient, Chase, EY, YOH, ...   \n",
       "3                     SEI, Outside, Vertex Inc, King   \n",
       "4  HomeAdvisor, Angi, Van Heusen, SAP, Google, LL...   \n",
       "\n",
       "                                           Job_Title  \\\n",
       "0  Inside Sales Representative, Sales Development...   \n",
       "1  Operations Analyst, Data Analyst, Internationa...   \n",
       "2  Data Analyst, Senior Recruiter, Financial Analyst   \n",
       "3  Inside Sales Representative, Data Entry, Solut...   \n",
       "4                                  Account Executive   \n",
       "\n",
       "                                              Skills  \n",
       "0  Partnership, Sales, Crm, Oracle, Process Impro...  \n",
       "1  Operations, Sales, Transactions, Warehouse, Qu...  \n",
       "2  Machine Learning, Mining, Mis, Python, Sports,...  \n",
       "3  Sales, Data Entry, Oracle, Value Proposition, ...  \n",
       "4  Staffing, Sales, Crm, Oracle, Retention, Video...  "
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7cb0b22e99bf12244e73905db9997166c9cd7dd3da0cb916400675d967e1fa08"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
